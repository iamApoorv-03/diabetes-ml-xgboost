{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751bb370-3f9e-4dbe-8c6f-ef252e5e5308",
   "metadata": {},
   "source": [
    "## Phase 3: Cross-Validated Baseline Modeling and Comprehensive Error Analysis — Logistic Regression & Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d107fc-b026-4436-a460-1298488d1f39",
   "metadata": {},
   "source": [
    "#### Categorical Feature Handling Before Baseline Modeling\n",
    "\n",
    "During exploratory analysis, I found that only two columns in the dataset—`age_group` and `bmi_class`—were categorical segment features. In initial modeling (Logistic Regression and Random Forest), I dropped these columns for the following reasons:\n",
    "\n",
    "- Both features were engineered as categorical \"bins\" from continuous variables (Age, BMI).\n",
    "- Preliminary tests and domain context suggested these categories added little discriminative value and could introduce redundancy or multicollinearity.\n",
    "- Tree-based models and Logistic Regression performed adequately on the main continuous features and core predictors.\n",
    "- Retaining a clean, fully numerical feature set simplifies comparison across models and ensures consistency for SVM or other algorithms requiring numeric input.\n",
    "\n",
    "For SVM and advanced model workflows, I continue to drop these segment categorical features to maintain a consistent, reproducible modeling pipeline and maximize performance on purely numerical variables.\n",
    "\n",
    "*Note: If future analysis or model interpretability suggests value in categorical segmentation, these features can be reintroduced using appropriate encoding techniques (e.g., one-hot encoding).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ed281-d57b-4aba-87be-589e645bf974",
   "metadata": {},
   "source": [
    "\n",
    "### Baseline Model 1: Logistic Regression — Cross-Validated Performance & Error Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b23da74-b484-4ed8-a9e2-a19ba7fd74ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_scaled2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Data prep\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m X_logreg = df_scaled2.drop(\u001b[33m'\u001b[39m\u001b[33mOutcome\u001b[39m\u001b[33m'\u001b[39m, axis=\u001b[32m1\u001b[39m).select_dtypes(include=[np.number])\n\u001b[32m     12\u001b[39m y_logreg = df_scaled2[\u001b[33m'\u001b[39m\u001b[33mOutcome\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     14\u001b[39m logreg = LogisticRegression(max_iter=\u001b[32m500\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_scaled2' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay,\n",
    "    precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data prep\n",
    "X_logreg = df_scaled2.drop('Outcome', axis=1).select_dtypes(include=[np.number])\n",
    "y_logreg = df_scaled2['Outcome']\n",
    "\n",
    "logreg = LogisticRegression(max_iter=500, random_state=42)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validated predictions and probabilities\n",
    "y_pred_logreg = cross_val_predict(logreg, X_logreg, y_logreg, cv=cv)\n",
    "y_prob_logreg = cross_val_predict(logreg, X_logreg, y_logreg, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Confusion matrix\n",
    "cm_logreg = confusion_matrix(y_logreg, y_pred_logreg)\n",
    "ConfusionMatrixDisplay(cm_logreg).plot(cmap='Blues')\n",
    "plt.title('Logistic Regression Confusion Matrix (5-Fold Cross-Validated)')\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "RocCurveDisplay.from_predictions(y_logreg, y_prob_logreg)\n",
    "plt.title('Logistic Regression ROC Curve (5-Fold Cross-Validated)')\n",
    "plt.show()\n",
    "\n",
    "# Metrics\n",
    "precision = precision_score(y_logreg, y_pred_logreg)\n",
    "recall = recall_score(y_logreg, y_pred_logreg)\n",
    "f1 = f1_score(y_logreg, y_pred_logreg)\n",
    "accuracy = accuracy_score(y_logreg, y_pred_logreg)\n",
    "roc_auc = roc_auc_score(y_logreg, y_prob_logreg)\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1-score:  {f1:.3f}\")\n",
    "print(f\"Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"ROC AUC:   {roc_auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71861dec-323b-47b4-b6b9-fdb1d15ea406",
   "metadata": {},
   "source": [
    "### Results Interpreted from Logistic Regression\n",
    "\n",
    "The Logistic Regression model was evaluated using 5-fold cross-validation with multiple metrics:\n",
    "\n",
    "- **ROC AUC (0.833):**\n",
    "  - The model's overall ability to distinguish between positive and negative classes is quite good, as shown by the high ROC AUC. This indicates strong general separability, but ROC AUC does not directly reflect real-world tradeoffs between recall and precision at a given threshold.\n",
    "- **Recall (Sensitivity, 0.567):**\n",
    "  - The model is only able to correctly identify about 57% of the actual disease cases (true positives). In other words, 43% of positive cases are missed (false negatives). This is particularly concerning for disease diagnosis, where the cost of missing a patient with a condition is typically high.\n",
    "- **Precision (0.682):**\n",
    "  - Among all cases predicted to have the disease, nearly 68% are correct. This tells us that positive predictions are reasonably reliable, but there are still a significant number of false positives.\n",
    "- **F1-score (0.619):**\n",
    "  - This metric provides a balance between precision and recall, but as both are moderate, the F1-score is only 0.62—reflecting the current model's struggle to maximize both sensitivity and specificity.\n",
    "- **Accuracy (0.757):**\n",
    "  - While the accuracy appears quite strong, it is not reliable for imbalanced datasets, and should not be used as a primary evaluation metric in this scenario.\n",
    "- **Confusion Matrix:**\n",
    "  - The confusion matrix reveals 116 false negatives (missed real cases). In a medical context, these can have serious or even fatal consequences. There are also 71 false positives, meaning some healthy individuals are misclassified as diseased.\n",
    "- **Summary:**  \n",
    "  - **Key Concern:** The moderate recall and number of false negatives mean many individuals who actually have the disease might go undetected if this model is used as-is.\n",
    "  - **Clinical Implication:** For disease screening, missing a positive is much worse than a false alarm. Therefore, recall should be maximized, and the current results suggest more improvement is required.\n",
    "\n",
    "---\n",
    "\n",
    "### Next: Random Forest Comparison\n",
    "\n",
    "To better understand the strengths and weaknesses of our baseline, the next step is to:\n",
    "- **Apply the same cross-validated workflow to a Random Forest classifier.**\n",
    "- Compare the ROC curve, recall, precision, F1-score, and confusion matrix with those of logistic regression to evaluate whether Random Forest can better capture the minority class (disease cases) and improve recall.\n",
    "\n",
    "---\n",
    "\n",
    "### After Comparing Models\n",
    "\n",
    "Regardless of which baseline performs slightly better, both could benefit from model tuning and imbalance strategies:\n",
    "1. **Threshold Tuning:** Adjust the probability threshold downward (from 0.5 to e.g. 0.4 or lower) and review the effect on recall and precision using a precision-recall curve.\n",
    "2. **Class Imbalance Solutions:** Try class weighting (`class_weight='balanced'`), SMOTE/oversampling the minority class, or undersampling the majority.\n",
    "3. **Additional Modeling:** Explore ensemble techniques or other classifiers (XGBoost, LightGBM) and tune hyperparameters.\n",
    "4. **Feature Engineering:** Investigate new features, interactions, or non-linear transformations to better capture signal in positive cases.\n",
    "5. **Reporting and Interpretation:** Document both default and recall-optimized performance, including clinical risks and tradeoffs, for well-informed decision making.\n",
    "\n",
    "---\n",
    "\n",
    "_This process provides maximum transparency and sets the stage for robust, high-sensitivity disease prediction._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a2697-1ef3-4466-a2c3-6cf33f6df807",
   "metadata": {},
   "source": [
    "### Baseline Model 2: Random Forest — Cross-Validated Performance & Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b5350-3547-433a-922b-4bc71fc40a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay,\n",
    "    precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    ")### Baseline Model 2: Random Forest — Cross-Validated Performance & Error Analysis\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import matplotlib.pyplot as plt\n",
    "X_rf = df_scaled2.drop('Outcome', axis=1).select_dtypes(include=[np.number])\n",
    "y_rf = df_scaled2['Outcome']\n",
    "\n",
    "# Random Forest predictions and probabilities (cross-validated)\n",
    "y_pred_rf = cross_val_predict(rf, X_rf, y_rf, cv=cv)\n",
    "y_prob_rf = cross_val_predict(rf, X_rf, y_rf, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Confusion matrix\n",
    "cm_rf = confusion_matrix(y_rf, y_pred_rf)\n",
    "ConfusionMatrixDisplay(cm_rf).plot(cmap='Blues')\n",
    "plt.title(f'Random Forest Confusion Matrix ({cv}-Fold Cross-Validated)')\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "RocCurveDisplay.from_predictions(y_rf, y_prob_rf)\n",
    "plt.title(f'Random Forest ROC Curve ({cv}-Fold Cross-Validated)')\n",
    "plt.show()\n",
    "\n",
    "# Precision, recall, F1-score, accuracy, ROC AUC\n",
    "precision = precision_score(y_rf, y_pred_rf)\n",
    "recall = recall_score(y_rf, y_pred_rf)\n",
    "f1 = f1_score(y_rf, y_pred_rf)\n",
    "accuracy = accuracy_score(y_rf, y_pred_rf)\n",
    "roc_auc = roc_auc_score(y_rf, y_prob_rf)\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1-score:  {f1:.3f}\")\n",
    "print(f\"Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"ROC AUC:   {roc_auc:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe512c-b713-4b22-a940-d73ca7e5710a",
   "metadata": {},
   "source": [
    "### Baseline Model 2: Random Forest — Cross-Validated Performance & Error Analysis\n",
    "\n",
    "**Random Forest Model Results (5-fold Cross-Validation):**\n",
    "- **ROC AUC:** 0.815\n",
    "- **Recall (Sensitivity):** 0.597\n",
    "- **Precision:** 0.675\n",
    "- **F1-Score:** 0.634\n",
    "- **Accuracy:** 0.759\n",
    "\n",
    "#### Detailed Interpretation:\n",
    "- **ROC Curve and AUC (0.815):**  \n",
    "  The Random Forest demonstrates strong global discriminative power, only slightly lower than Logistic Regression. This shows it is effective at distinguishing positive and negative classes, but recall is still a limiting factor for clinical screening.\n",
    "\n",
    "- **Recall (0.597):**  \n",
    "  The recall, or sensitivity, is 0.597—higher than Logistic Regression. The model correctly detects approximately 60% of disease cases. This means about 40% of positives are missed (false negatives). While this is an improvement over Logistic Regression, it is still a key limitation in a disease diagnosis context.\n",
    "\n",
    "- **Precision (0.675):**  \n",
    "  Out of all the cases predicted as positive, about 67.5% are indeed diseased. Random Forest achieves slightly less precision than Logistic Regression, reflecting a tradeoff between catching more positives and increasing false alarms.\n",
    "\n",
    "- **F1-score (0.634):**  \n",
    "  Slightly higher than Logistic Regression, the F1-score balances precision and recall, indicating a marginally improved ability to identify positives while managing false positives.\n",
    "\n",
    "- **Accuracy (0.759):**  \n",
    "  Very similar to Logistic Regression, showing overall prediction alignment on this imbalanced data, but not the primary metric of interest.\n",
    "\n",
    "- **Confusion Matrix:**  \n",
    "  Random Forest produces 108 false negatives—fewer than Logistic Regression (116), so it misses fewer cases. It also predicts more true positives (160 vs. 152 for Logistic), again emphasizing its better sensitivity.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Comparison\n",
    "\n",
    "| Metric        | Logistic Regression | Random Forest   |\n",
    "|---------------|--------------------|----------------|\n",
    "| **ROC AUC**   | 0.833              | 0.815          |\n",
    "| **Recall**    | 0.567              | 0.597          |\n",
    "| **Precision** | 0.682              | 0.675          |\n",
    "| **F1-score**  | 0.619              | 0.634          |\n",
    "| **Accuracy**  | 0.757              | 0.759          |\n",
    "| **False Neg.**| 116                | 108            |\n",
    "| **True Pos.** | 152                | 160            |\n",
    "\n",
    "- **Takeaways:**  \n",
    "  - **Random Forest** gives higher recall and F1-score than Logistic Regression, making it safer for clinical use where missing a positive is costly.\n",
    "  - **Logistic Regression** produces slightly higher precision and ROC AUC.\n",
    "  - Both models leave room for improvement, especially in recall.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Threshold Tuning for Higher Recall**\n",
    "    - Lower the classification threshold for Random Forest (and Logistic Regression if desired), plotting precision-recall curves and selecting a threshold that delivers the desired sensitivity for the positive class.\n",
    "\n",
    "2. **Class Imbalance Handling**\n",
    "    - Use `class_weight='balanced'` in both models, or apply SMOTE/oversampling of the minority class and compare results.\n",
    "    - Try under-sampling the majority class as another balancing method.\n",
    "\n",
    "3. **Advanced Modeling**\n",
    "    - Try ensemble models like XGBoost or LightGBM, or stack multiple classifiers for better robustness.\n",
    "    - Tune hyperparameters for both Random Forest and Logistic Regression to optimize recall.\n",
    "\n",
    "4. **Feature Engineering**\n",
    "    - Investigate adding or transforming features to highlight signals present in positive cases.\n",
    "\n",
    "5. **Reporting and Clinical Review**\n",
    "    - Summarize recall-optimized results with confusion matrices and risk tradeoffs.\n",
    "    - Collaborate with clinical stakeholders to determine the optimal recall/precision tradeoff for practical deployment.\n",
    "\n",
    "---\n",
    "\n",
    "_By pursuing these steps, you'll move from baselines to a recall-optimized, clinically defensible screening model._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ef19e5-bdfb-48f9-afda-2da1c3745840",
   "metadata": {},
   "source": [
    "### Step 1: Threshold Tuning for Maximum Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d0fde3-dc42-4dfa-9f57-ecbae1c6de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Example if your DataFrame is df_scaled2 and target is 'Outcome'\n",
    "X_rf = df_scaled2.drop('Outcome', axis=1).select_dtypes(include=[np.number])\n",
    "y_rf = df_scaled2['Outcome']\n",
    "\n",
    "\n",
    "# Predict probabilities (already from cross_val_predict if desired, else fit and predict_proba)\n",
    "y_prob_rf = cross_val_predict(rf, X_rf, y_rf, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Sweep thresholds and collect metrics\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "recalls, precisions, fscores, accuracies = [], [], [], []\n",
    "\n",
    "for thr in thresholds:\n",
    "    y_pred_thr = (y_prob_rf >= thr).astype(int)\n",
    "    recalls.append(recall_score(y_rf, y_pred_thr))\n",
    "    precisions.append(precision_score(y_rf, y_pred_thr))\n",
    "    fscores.append(f1_score(y_rf, y_pred_thr))\n",
    "    accuracies.append(accuracy_score(y_rf, y_pred_thr))\n",
    "\n",
    "# Plot recall and precision vs threshold\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(thresholds, recalls, label='Recall', marker='o')\n",
    "plt.plot(thresholds, precisions, label='Precision', marker='x')\n",
    "plt.plot(thresholds, fscores, label='F1-score', marker='^')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', marker='s')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Metric Curves vs. Threshold (Random Forest)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# You may choose a threshold with higher recall (even if precision drops slightly)\n",
    "best_idx = np.argmax(recalls)\n",
    "print(f'Best Recall: {recalls[best_idx]:.3f} at Threshold: {thresholds[best_idx]:.2f} (Precision: {precisions[best_idx]:.3f}, F1: {fscores[best_idx]:.3f}, Accuracy: {accuracies[best_idx]:.3f})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad75066-910c-4238-91de-8f8912553a1d",
   "metadata": {},
   "source": [
    "#### 1. What We Interpreted (from Random Forest threshold tuning)\n",
    "\n",
    "- Lowering the decision threshold rapidly increases recall (sensitivity) but at the cost of reducing precision and overall accuracy.\n",
    "- At threshold = 0.10 for Random Forest:\n",
    "  - **Recall reached 0.974** (very high: almost all true cases flagged)\n",
    "  - **Precision dropped to 0.44**, and accuracy to ~0.56 (most flagged cases were false alarms, a result of aggressive recall)\n",
    "- **Takeaway:**  \n",
    "   - This tradeoff may be justified for early disease screening (where missing positives is unacceptable), but creates workload/cost/psychological burden for unnecessary follow-up testing.\n",
    "   - For diagnosis (not just screening), this model and threshold would be too imprecise.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Threshold Tuning & Plotting for Logistic Regression\n",
    "\n",
    "Now, we apply the same threshold tuning approach to **Logistic Regression**. The goal:  \n",
    "- Visualize how recall, precision, F1, and accuracy change as the threshold is swept.\n",
    "- Decide what tradeoff you can reasonably accept.\n",
    "\n",
    "Place this code in your notebook:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b456b-f26a-43a0-a11d-531e180228fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit and get probabilities for logistic regression (make sure X_logreg, y_logreg, and cv are correctly defined)\n",
    "logreg = LogisticRegression(max_iter=500, random_state=42)\n",
    "y_prob_logreg = cross_val_predict(logreg, X_logreg, y_logreg, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Sweep thresholds and collect metrics\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "recalls, precisions, fscores, accuracies = [], [], [], []\n",
    "\n",
    "for thr in thresholds:\n",
    "    y_pred_thr = (y_prob_logreg >= thr).astype(int)\n",
    "    recalls.append(recall_score(y_logreg, y_pred_thr))\n",
    "    precisions.append(precision_score(y_logreg, y_pred_thr))\n",
    "    fscores.append(f1_score(y_logreg, y_pred_thr))\n",
    "    accuracies.append(accuracy_score(y_logreg, y_pred_thr))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(thresholds, recalls, label='Recall', marker='o')\n",
    "plt.plot(thresholds, precisions, label='Precision', marker='x')\n",
    "plt.plot(thresholds, fscores, label='F1-score', marker='^')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', marker='s')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Metric Curves vs. Threshold (Logistic Regression)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "best_idx = np.argmax(recalls)\n",
    "print(f'Best Recall: {recalls[best_idx]:.3f} at Threshold: {thresholds[best_idx]:.2f} (Precision: {precisions[best_idx]:.3f}, F1: {fscores[best_idx]:.3f}, Accuracy: {accuracies[best_idx]:.3f})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573880fc-4d19-4839-8c12-e50a86e707bd",
   "metadata": {},
   "source": [
    "### Logistic Regression: Threshold Tuning Analysis\n",
    "\n",
    "#### 1. Metric Curves vs. Threshold\n",
    "\n",
    "- **Best Recall: 0.974 at Threshold: 0.10**  \n",
    "  (Precision: 0.453, F1: 0.618, Accuracy: 0.581)\n",
    "\n",
    "#### 2. Interpretation\n",
    "\n",
    "- Lowering the threshold to 0.10 causes:\n",
    "    - **Recall** (sensitivity) to reach 0.974, meaning the model successfully identifies nearly all true positive (disease) cases.\n",
    "    - **Precision** drops to 0.453: less than fifty percent of flagged positives are true positives, producing many false alarms.\n",
    "    - **F1-score** is moderate (0.618). This reflects the tradeoff: you’re catching almost all cases, but with many false alarms.\n",
    "    - **Accuracy** also drops—most likely due to class imbalance and the high number of false positives.\n",
    "- The shape of the curves is similar to Random Forest: **as threshold decreases, recall increases but precision and accuracy decrease.**\n",
    "- **Takeaway:** This setting maximizes sensitivity for use as a screening tool. However, just as with Random Forest, too many false alarms might burden the clinical workflow, leading to unnecessary follow-ups.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. What This Means\n",
    "\n",
    "- **Good Use:**  \n",
    "    - **Early Screening or Triage:** When the cost of missing a true case is very high and downstream confirmation is feasible.\n",
    "- **Potential Drawback:**  \n",
    "    - **Diagnostic Phase:** If positive cases flagged by the model always required an expensive or invasive test, precision this low could cause too many unnecessary procedures.\n",
    "    - **Resource Management:** High false positive rate could overwhelm limited medical resources.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Recommended Next Steps\n",
    "\n",
    "- **Clinically Informed Threshold Selection:**  \n",
    "   - Rather than using the \"best recall\" threshold by default, discuss with clinicians to identify a tradeoff that keeps recall high but improves precision/accuracy where possible (e.g., perhaps 0.3–0.5).\n",
    "- **Balance Techniques:**  \n",
    "   - Try SMOTE or class weighting to bring up recall **and** precision together. \n",
    "- **Model Comparison:**  \n",
    "   - Compare these curves and tradeoffs to those from Random Forest. If the shapes are very similar, further model/feature work may be needed to get both high recall and improved precision.\n",
    "- **Experiment with advanced models:**  \n",
    "   - Use XGBoost, LightGBM, CatBoost, or ensemble approaches to try to shift the tradeoff.\n",
    "- **Stakeholder Review:**  \n",
    "   - Present clear curves, confusion matrices, and impact for threshold choices to non-technical decision makers.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "Just like Random Forest, your logistic regression model can be made extremely sensitive but non-specific. This is useful for “ruling out” in first-line screening, but you need a follow-up plan for resolving false alarms and ensuring resources are used intelligently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76237703-5470-429b-8219-e6d58894df4f",
   "metadata": {},
   "source": [
    "#### Selecting a Clinically Sensible Threshold Based on Recall Target(Logistic regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be05b565-f7ef-4e9f-a292-e2af95947d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you want to choose the threshold where recall >= 0.90 (edit value if you want slightly more/less recall)\n",
    "desired_recall = 0.90\n",
    "\n",
    "# Find the index where recall just crosses this threshold\n",
    "recall_idxs = np.where(np.array(recalls) >= desired_recall)[0]\n",
    "\n",
    "if len(recall_idxs) > 0:\n",
    "    idx = recall_idxs[-1]  # The highest threshold for this recall (i.e., safest)\n",
    "    selected_thr = thresholds[idx]\n",
    "    print(f\"Selected Threshold: {selected_thr:.2f}\")\n",
    "    print(f\"Recall:    {recalls[idx]:.3f}\")\n",
    "    print(f\"Precision: {precisions[idx]:.3f}\")\n",
    "    print(f\"F1-score:  {fscores[idx]:.3f}\")\n",
    "    print(f\"Accuracy:  {accuracies[idx]:.3f}\")\n",
    "else:\n",
    "    print(\"No threshold achieves the desired recall target. Try lowering the target.\")\n",
    "\n",
    "# If you want to see which threshold gives little loss of precision for a still good recall, you can also sweep for higher F1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951b728-aa6b-4d39-9cd7-9e69a2d60cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Use the same probabilities and data as before\n",
    "selected_thr = 0.18\n",
    "y_pred_selected = (y_prob_logreg >= selected_thr).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_logreg, y_pred_selected)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(f\"Logistic Regression Confusion Matrix (Threshold = {selected_thr})\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print underlying confusion matrix values\n",
    "print(\"Confusion Matrix (rows=true, cols=pred):\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d8d0d-3258-4828-9a56-2633f2f0b95b",
   "metadata": {},
   "source": [
    "#### Logistic Regression Confusion Matrix at Selected Clinical Threshold (Threshold = 0.18)\n",
    "\n",
    "|                | Predicted Negative | Predicted Positive |\n",
    "|----------------|-------------------|-------------------|\n",
    "| **True Negative** | 270               | 230               |\n",
    "| **True Positive** | 22                | 246               |\n",
    "\n",
    "- **True Positive (TP):** 246 — Actual positives correctly identified.\n",
    "- **False Positive (FP):** 230 — Actual negatives incorrectly flagged as positives (false alarms).\n",
    "- **True Negative (TN):** 270 — Actual negatives correctly identified.\n",
    "- **False Negative (FN):** 22 — Actual positives missed by the model.\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:**\n",
    "- **Recall** remains very high: only **22 actual positive cases are missed**.\n",
    "- **Precision** is moderate, reflecting that a little more than half your positive predictions are correct; many are false alarms.\n",
    "- There is a significant number of **false positives (230)**, which is the cost of achieving high recall.\n",
    "\n",
    "**Clinical Implication:**  \n",
    "At this threshold, your model is appropriate for scenarios where **missing a real case is unacceptable** (screening), and the healthcare/clinical system can handle false positives for further testing or triage.\n",
    "\n",
    "---\n",
    "\n",
    "**Next suggestion**:  \n",
    "- Repeat this same confusion matrix analysis for **Random Forest** at its chosen recall-optimized threshold, or\n",
    "- Begin with class balancing (SMOTE/class weights) for further improvement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf21d9c-89f4-4754-b839-a8dd7f9b1aa2",
   "metadata": {},
   "source": [
    "### Selecting a Clinically Sensible Threshold Based on Recall Target (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01efbd07-341a-4bb6-9421-5e55cc22e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_recall_rf = 0.90  # or your chosen high value\n",
    "\n",
    "recall_idxs_rf = np.where(np.array(recalls) >= desired_recall_rf)[0]\n",
    "\n",
    "if len(recall_idxs_rf) > 0:\n",
    "    idx_rf = recall_idxs_rf[-1]  # The highest threshold for this recall\n",
    "    selected_thr_rf = thresholds[idx_rf]\n",
    "    print(f\"Selected Threshold (Random Forest): {selected_thr_rf:.2f}\")\n",
    "    print(f\"Recall:    {recalls[idx_rf]:.3f}\")\n",
    "    print(f\"Precision: {precisions[idx_rf]:.3f}\")\n",
    "    print(f\"F1-score:  {fscores[idx_rf]:.3f}\")\n",
    "    print(f\"Accuracy:  {accuracies[idx_rf]:.3f}\")\n",
    "else:\n",
    "    print(\"No threshold achieves the desired recall target for Random Forest. Try lowering the target.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31001e46-dd69-40fa-b758-fbcc7b65153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Use the same predicted probabilities and true labels as before (for Random Forest)\n",
    "y_pred_rf_selected = (y_prob_rf >= 0.18).astype(int)\n",
    "\n",
    "cm_rf = confusion_matrix(y_rf, y_pred_rf_selected)\n",
    "disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf)\n",
    "disp_rf.plot(cmap='Blues')\n",
    "plt.title(\"Random Forest Confusion Matrix (Threshold = 0.18)\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print raw counts for summary\n",
    "print(\"Random Forest Confusion Matrix (rows=true, cols=pred):\\n\", cm_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0cce99-3eda-4656-980a-66c62f05700e",
   "metadata": {},
   "source": [
    "#### Random Forest vs Logistic Regression at Clinically-Informed Threshold (Threshold = 0.18)\n",
    "\n",
    "##### **Confusion Matrices**\n",
    "\n",
    "| Model                | True Neg (TN) | False Pos (FP) | False Neg (FN) | True Pos (TP) |\n",
    "|----------------------|:-------------:|:--------------:|:--------------:|:-------------:|\n",
    "| Logistic Regression  |     270       |      230       |      22        |     246       |\n",
    "| Random Forest        |     242       |      258       |      25        |     243       |\n",
    "\n",
    "##### **Metrics**\n",
    "\n",
    "| Model                | Recall | Precision | F1-score | Accuracy |\n",
    "|----------------------|:------:|:---------:|:--------:|:--------:|\n",
    "| Logistic Regression  | 0.918  |   0.517   |  0.661   |  0.672   |\n",
    "| Random Forest        | 0.907  |   0.485   |  0.632   |  0.628   |\n",
    "\n",
    "---\n",
    "\n",
    "##### **Interpretation & Comparison**\n",
    "\n",
    "- **Recall:** Both models achieve very high recall, slightly higher with Logistic Regression (0.918) than Random Forest (0.907), meaning both miss very few actual positive cases.\n",
    "- **Precision:** Both models have moderate precision, with Logistic being modestly higher—just over half of positive predictions are correct.\n",
    "- **False Positives:** Random Forest produces more false positives (258) than Logistic Regression (230), so Logistic Regression will generate fewer unnecessary alarms for the same recall level.\n",
    "- **F1-score & Accuracy:** Both metrics are slightly higher for Logistic Regression. Both models, however, still have a large number of false positives as the cost for high recall.\n",
    "- **Clinical Use:** Both thresholds are appropriate for first-stage screening if the system can handle the follow-up demand for false positives. Logistic Regression may be preferred if minimizing false alarms is a top concern, but the difference is modest.\n",
    "\n",
    "---\n",
    "\n",
    "##### **Next Steps**\n",
    "\n",
    "- Consider class balancing (SMOTE, class weights) to improve precision.\n",
    "- Explore advanced models to shift the tradeoff curve.\n",
    "- Present this side-by-side summary to clinical or managerial stakeholders for final workflow decision.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ace4f-4b79-47dd-8fa9-016c47659fa6",
   "metadata": {},
   "source": [
    "### Step: Class Balancing to Improve Recall-Precision Tradeoff\n",
    "\n",
    "**Objective:**  \n",
    "To reduce the number of false positives while maintaining high recall by handling class imbalance using:\n",
    "- 1. `class_weight='balanced'` in model initialization (LogisticRegression, RandomForestClassifier)\n",
    "- 2. Synthetic Minority Over-sampling Technique (SMOTE)\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Using Class Weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc2de6a-bbbc-4752-a5c7-39b7ea8c52d1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### 2. Using SMOTE (Synthetic Oversampling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0254ab-f253-47f2-b789-fffc5ad6b5d8",
   "metadata": {},
   "source": [
    "\n",
    "*Train new models on these resampled datasets using the same pipeline as before.*\n",
    "\n",
    "---\n",
    "\n",
    "#### Next Actions\n",
    "\n",
    "1. Repeat your **metric curve plotting and threshold selection** process for these rebalanced models.\n",
    "2. Compare confusion matrices and metrics to previous results.\n",
    "3. Interpret: *Does class balancing increase precision while keeping recall high? Does it significantly change F1-score and accuracy?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cafb2-899b-4170-bc8b-24abb7ad8fc8",
   "metadata": {},
   "source": [
    "### 1) Using Class Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a155e-cfe5-4f52-8bf5-de6089426038",
   "metadata": {},
   "source": [
    "### Logistic Regression with Class Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ddcdbb-d42d-4896-a1f7-e0ed777f77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Model with class_weight\n",
    "logreg_bal = LogisticRegression(max_iter=500, class_weight='balanced', random_state=42)\n",
    "y_prob_logreg_bal = cross_val_predict(logreg_bal, X_logreg, y_logreg, cv=cv, method='predict_proba')[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89846fc7-f89a-4006-a39f-f099eaf65bc2",
   "metadata": {},
   "source": [
    "### Metric Curve Plotting for Balanced Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd1ddb-db9c-427e-adcb-830e01ea2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "recalls_bal, precisions_bal, fscores_bal, accuracies_bal = [], [], [], []\n",
    "\n",
    "for thr in thresholds:\n",
    "    y_pred_thr = (y_prob_logreg_bal >= thr).astype(int)\n",
    "    recalls_bal.append(recall_score(y_logreg, y_pred_thr))\n",
    "    precisions_bal.append(precision_score(y_logreg, y_pred_thr))\n",
    "    fscores_bal.append(f1_score(y_logreg, y_pred_thr))\n",
    "    accuracies_bal.append(accuracy_score(y_logreg, y_pred_thr))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(thresholds, recalls_bal, label='Recall', marker='o')\n",
    "plt.plot(thresholds, precisions_bal, label='Precision', marker='x')\n",
    "plt.plot(thresholds, fscores_bal, label='F1-score', marker='^')\n",
    "plt.plot(thresholds, accuracies_bal, label='Accuracy', marker='s')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Metric Curves vs. Threshold (LogReg with class_weight=\"balanced\")')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7098d-ae8f-4861-936f-1ecf21b9a865",
   "metadata": {},
   "source": [
    "### Selecting Sensible Threshold and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7cbc5-e284-4c5c-9e81-2f00850508ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select threshold for recall >= 0.90 (or what you prefer)\n",
    "desired_recall = 0.90\n",
    "recall_idxs_bal = np.where(np.array(recalls_bal) >= desired_recall)[0]\n",
    "\n",
    "if len(recall_idxs_bal) > 0:\n",
    "    idx_bal = recall_idxs_bal[-1]\n",
    "    selected_thr_bal = thresholds[idx_bal]\n",
    "    print(f\"Selected Threshold: {selected_thr_bal:.2f}\")\n",
    "    print(f\"Recall:    {recalls_bal[idx_bal]:.3f}\")\n",
    "    print(f\"Precision: {precisions_bal[idx_bal]:.3f}\")\n",
    "    print(f\"F1-score:  {fscores_bal[idx_bal]:.3f}\")\n",
    "    print(f\"Accuracy:  {accuracies_bal[idx_bal]:.3f}\")\n",
    "else:\n",
    "    print(\"No threshold achieves the desired recall target.\")\n",
    "\n",
    "# Show confusion matrix at this threshold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "y_pred_bal = (y_prob_logreg_bal >= selected_thr_bal).astype(int)\n",
    "cm_bal = confusion_matrix(y_logreg, y_pred_bal)\n",
    "disp_bal = ConfusionMatrixDisplay(confusion_matrix=cm_bal)\n",
    "disp_bal.plot(cmap='Blues')\n",
    "plt.title(f\"Confusion Matrix (LogReg with class_weight, Threshold={selected_thr_bal:.2f})\")\n",
    "plt.show()\n",
    "print(\"Confusion Matrix (rows=true, cols=pred):\\n\", cm_bal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd3418-89ae-4f4f-bc94-66173efd70ac",
   "metadata": {},
   "source": [
    "#### Logistic Regression (with class_weight='balanced', Threshold = 0.29): Results\n",
    "\n",
    "- **Selected Threshold:** 0.29\n",
    "- **Recall:** 0.907\n",
    "- **Precision:** 0.517\n",
    "- **F1-score:** 0.659\n",
    "- **Accuracy:** 0.672\n",
    "\n",
    "|                | Predicted Negative | Predicted Positive |\n",
    "|----------------|-------------------|-------------------|\n",
    "| **True Negative** | 273               | 227               |\n",
    "| **True Positive** | 25                | 243               |\n",
    "\n",
    "- **True Positive (TP):** 243\n",
    "- **False Positive (FP):** 227\n",
    "- **True Negative (TN):** 273\n",
    "- **False Negative (FN):** 25\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:**\n",
    "- With class balancing by class_weight, logistic regression maintains high recall while **reducing false positives and increasing true negatives** compared to the vanilla model.\n",
    "- **Precision and overall accuracy improve or are maintained**; you have successfully preserved sensitivity while gaining on the specificity side.\n",
    "- This means fewer unnecessary follow-ups for a very similar miss rate on real cases.\n",
    "\n",
    "**Clinical implication:**  \n",
    "With this setting, WE have an even more practical screening model, balancing the need to catch nearly all true cases without overwhelming the follow-up system.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7087176-4a8a-4611-9625-edd21c7b4e2f",
   "metadata": {},
   "source": [
    "### Random Forest with Class Weight (class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a6bb9d-3af6-4786-92e9-7c8dfd2eeff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Train Random Forest with class_weight='balanced'\n",
    "rf_bal = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "y_prob_rf_bal = cross_val_predict(rf_bal, X_rf, y_rf, cv=cv, method='predict_proba')[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8da378-6ed1-40f7-816c-b38883343781",
   "metadata": {},
   "source": [
    "### Metric Curve Plotting for Balanced Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e5e31-fdf7-4b28-b112-006bb8070c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "recalls_rf_bal, precisions_rf_bal, fscores_rf_bal, accuracies_rf_bal = [], [], [], []\n",
    "\n",
    "for thr in thresholds:\n",
    "    y_pred_thr = (y_prob_rf_bal >= thr).astype(int)\n",
    "    recalls_rf_bal.append(recall_score(y_rf, y_pred_thr))\n",
    "    precisions_rf_bal.append(precision_score(y_rf, y_pred_thr))\n",
    "    fscores_rf_bal.append(f1_score(y_rf, y_pred_thr))\n",
    "    accuracies_rf_bal.append(accuracy_score(y_rf, y_pred_thr))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(thresholds, recalls_rf_bal, label='Recall', marker='o')\n",
    "plt.plot(thresholds, precisions_rf_bal, label='Precision', marker='x')\n",
    "plt.plot(thresholds, fscores_rf_bal, label='F1-score', marker='^')\n",
    "plt.plot(thresholds, accuracies_rf_bal, label='Accuracy', marker='s')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Metric Curves vs. Threshold (Random Forest with class_weight=\"balanced\")')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b23824-3740-41b7-8583-af57f1ad9acf",
   "metadata": {},
   "source": [
    "### Selecting Sensible Threshold and Confusion Matrix (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c66e21-117c-4c76-91ef-15dbd31461af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold selection for recall >= 0.90 (or your preferred value)\n",
    "desired_recall_rf = 0.90\n",
    "recall_idxs_rf_bal = np.where(np.array(recalls_rf_bal) >= desired_recall_rf)[0]\n",
    "\n",
    "if len(recall_idxs_rf_bal) > 0:\n",
    "    idx_rf_bal = recall_idxs_rf_bal[-1]\n",
    "    selected_thr_rf_bal = thresholds[idx_rf_bal]\n",
    "    print(f\"Selected Threshold: {selected_thr_rf_bal:.2f}\")\n",
    "    print(f\"Recall:    {recalls_rf_bal[idx_rf_bal]:.3f}\")\n",
    "    print(f\"Precision: {precisions_rf_bal[idx_rf_bal]:.3f}\")\n",
    "    print(f\"F1-score:  {fscores_rf_bal[idx_rf_bal]:.3f}\")\n",
    "    print(f\"Accuracy:  {accuracies_rf_bal[idx_rf_bal]:.3f}\")\n",
    "else:\n",
    "    print(\"No threshold achieves the desired recall target.\")\n",
    "\n",
    "# Confusion matrix at the selected threshold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "y_pred_rf_bal = (y_prob_rf_bal >= selected_thr_rf_bal).astype(int)\n",
    "cm_rf_bal = confusion_matrix(y_rf, y_pred_rf_bal)\n",
    "disp_rf_bal = ConfusionMatrixDisplay(confusion_matrix=cm_rf_bal)\n",
    "disp_rf_bal.plot(cmap='Blues')\n",
    "plt.title(f\"Confusion Matrix (RF with class_weight, Threshold={selected_thr_rf_bal:.2f})\")\n",
    "plt.show()\n",
    "print(\"Confusion Matrix (rows=true, cols=pred):\\n\", cm_rf_bal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007e24f-b4f2-4bf5-a44c-de2f2ebe7a2e",
   "metadata": {},
   "source": [
    "#### Logistic Regression (with class_weight='balanced', Threshold = 0.29): Results\n",
    "\n",
    "- **Selected Threshold:** 0.29\n",
    "- **Recall:** 0.907\n",
    "- **Precision:** 0.517\n",
    "- **F1-score:** 0.659\n",
    "- **Accuracy:** 0.672\n",
    "\n",
    "|                | Predicted Negative | Predicted Positive |\n",
    "|----------------|-------------------|-------------------|\n",
    "| **True Negative** | 273               | 227               |\n",
    "| **True Positive** | 25                | 243               |\n",
    "\n",
    "- **True Positive (TP):** 243\n",
    "- **False Positive (FP):** 227\n",
    "- **True Negative (TN):** 273\n",
    "- **False Negative (FN):** 25\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:**\n",
    "- With class balancing by class_weight, logistic regression maintains high recall while **reducing false positives and increasing true negatives** compared to the vanilla model.\n",
    "- **Precision and overall accuracy improve or are maintained**; you have successfully preserved sensitivity while gaining on the specificity side.\n",
    "- This means fewer unnecessary follow-ups for a very similar miss rate on real cases.\n",
    "\n",
    "**Clinical implication:**  \n",
    "With this setting, you have an even more practical screening model, balancing the need to catch nearly all true cases without overwhelming the follow-up system.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c5fa8-2dfc-4209-995b-3c3aef35a73f",
   "metadata": {},
   "source": [
    "#### Class-Weighted Models: Logistic Regression vs Random Forest (at Sensible Clinical Threshold)\n",
    "\n",
    "##### Logistic Regression (`class_weight='balanced'`, Threshold = 0.29)\n",
    "- **Recall:** 0.907\n",
    "- **Precision:** 0.517\n",
    "- **F1-score:** 0.659\n",
    "- **Accuracy:** 0.672\n",
    "\n",
    "|                | Predicted Negative | Predicted Positive |\n",
    "|----------------|-------------------|-------------------|\n",
    "| **True Negative** | 273               | 227               |\n",
    "| **True Positive** | 25                | 243               |\n",
    "\n",
    "##### Random Forest (`class_weight='balanced'`, Threshold = 0.20)\n",
    "- **Recall:** 0.907\n",
    "- **Precision:** 0.520\n",
    "- **F1-score:** 0.661\n",
    "- **Accuracy:** 0.676\n",
    "\n",
    "|                | Predicted Negative | Predicted Positive |\n",
    "|----------------|-------------------|-------------------|\n",
    "| **True Negative** | 276               | 224               |\n",
    "| **True Positive** | 25                | 243               |\n",
    "\n",
    "---\n",
    "\n",
    "##### **Interpretation & Comparison**\n",
    "\n",
    "- **Recall is identical (0.907) for both models**—both are excellent at catching true cases.\n",
    "- **Precision and F1-score are nearly the same**, with Random Forest being ever-so-slightly higher in both.\n",
    "- **Accuracy is slightly higher for Random Forest** (0.676 vs 0.672), mainly due to a few more correct negatives.\n",
    "- The *number of false positives and true negatives is very closely matched*; both models achieve a strong reduction in false positives versus their vanilla (unbalanced) forms.\n",
    "- **Clinical impact:** Either model is now both highly sensitive and much less overwhelming with unnecessary follow-ups thanks to balancing.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "- Both class-weighted models yield very similar, solid results at high recall.\n",
    "- **Random Forest edges ahead very slightly on precision, F1, and accuracy, but the difference is minor.**\n",
    "- You can confidently propose either model as an effective screening tool after class weighting.\n",
    "\n",
    "---\n",
    "\n",
    "**Next step:**  \n",
    "- Try SMOTE balancing to potentially squeeze out more improvement in minority class recovery or to test if even higher precision can be achieved without a recall drop.\n",
    "nm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc9fdd2-2798-4e09-a4f6-4dfca86baf81",
   "metadata": {},
   "source": [
    "#  Handling Class Imbalance Using SMOTE \n",
    "\n",
    "Real medical datasets often show a class imbalance, where the number of positive cases \n",
    "(diabetes diagnosed = 1) is much smaller than the number of negative cases.\n",
    "\n",
    "To ensure the model learns minority-class patterns effectively, we apply **SMOTE**  \n",
    "(Synthetic Minority Oversampling Technique) — but **only on the training data**, \n",
    "to avoid data leakage.\n",
    "\n",
    "In this section we will:\n",
    "\n",
    "1. Apply SMOTE safely (train data only)\n",
    "2. Scale the data after SMOTE\n",
    "3. Train and evaluate the following models:\n",
    "   - **Logistic Regression**\n",
    "   - **Random Forest**\n",
    "   - **XGBoost**\n",
    "4. Compare their performance on the untouched test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c854c-fcbe-43fd-830c-f24472db5a07",
   "metadata": {},
   "source": [
    "### Apply SMOTE Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0adf81-e9a7-4c88-adbf-24ba09b40752",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fd228-9f7b-439f-89be-6ae1edb9fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef126bd-81a5-4635-a69c-f8469ad4559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section: Handling Class Imbalance Using SMOTE\n",
    "# (Safe: Train-Only Oversampling to Avoid Data Leakage)\n",
    "# ============================================================\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score,\n",
    "    f1_score, accuracy_score, roc_auc_score, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Remove categorical / binned features before SMOTE\n",
    "# ------------------------------------------------------------\n",
    "df_smote_base = df_scaled2.drop(columns=[\"Age_Group\", \"BMI_Class\"])\n",
    "\n",
    "print(\"Columns used for SMOTE:\\n\", df_smote_base.columns.tolist())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Split into features & target\n",
    "# ------------------------------------------------------------\n",
    "X = df_smote_base.drop(columns=[\"Outcome\"])\n",
    "y = df_smote_base[\"Outcome\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Train-Test Split\n",
    "# ------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nTrain class distribution BEFORE SMOTE:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Apply SMOTE only on training data\n",
    "# ------------------------------------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nTrain class distribution AFTER SMOTE:\")\n",
    "print(y_train_res.value_counts())\n",
    "print(f\"Training samples after SMOTE: {X_train_res.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Scaling\n",
    "# ------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_res_s = scaler.fit_transform(X_train_res)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# ============================================================\n",
    "# 6. Logistic Regression + SMOTE\n",
    "# ============================================================\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_res_s, y_train_res)\n",
    "\n",
    "lr_probs = lr.predict_proba(X_test_s)[:, 1]\n",
    "lr_preds = (lr_probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n======= Logistic Regression + SMOTE =======\")\n",
    "print(\"Precision:\", precision_score(y_test, lr_preds))\n",
    "print(\"Recall:\", recall_score(y_test, lr_preds))\n",
    "print(\"F1:\", f1_score(y_test, lr_preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, lr_preds))\n",
    "print(\"AUC:\", roc_auc_score(y_test, lr_probs))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, lr_preds))\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, lr_preds)).plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Logistic Regression + SMOTE\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 7. Random Forest + SMOTE\n",
    "# ============================================================\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_res_s, y_train_res)\n",
    "\n",
    "rf_probs = rf.predict_proba(X_test_s)[:, 1]\n",
    "rf_preds = (rf_probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n======= Random Forest + SMOTE =======\")\n",
    "print(\"Precision:\", precision_score(y_test, rf_preds))\n",
    "print(\"Recall:\", recall_score(y_test, rf_preds))\n",
    "print(\"F1:\", f1_score(y_test, rf_preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_preds))\n",
    "print(\"AUC:\", roc_auc_score(y_test, rf_probs))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, rf_preds))\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, rf_preds)).plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Random Forest + SMOTE\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 8. XGBoost + SMOTE\n",
    "# ============================================================\n",
    "xgb = XGBClassifier(\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4\n",
    ")\n",
    "xgb.fit(X_train_res_s, y_train_res)\n",
    "\n",
    "xgb_probs = xgb.predict_proba(X_test_s)[:, 1]\n",
    "xgb_preds = (xgb_probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n======= XGBoost + SMOTE =======\")\n",
    "print(\"Precision:\", precision_score(y_test, xgb_preds))\n",
    "print(\"Recall:\", recall_score(y_test, xgb_preds))\n",
    "print(\"F1:\", f1_score(y_test, xgb_preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, xgb_preds))\n",
    "print(\"AUC:\", roc_auc_score(y_test, xgb_probs))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, xgb_preds))\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, xgb_preds)).plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - XGBoost + SMOTE\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 9. SVM (RBF Kernel) + SMOTE\n",
    "# ============================================================\n",
    "svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "svm.fit(X_train_res_s, y_train_res)\n",
    "\n",
    "svm_probs = svm.predict_proba(X_test_s)[:, 1]\n",
    "svm_preds = (svm_probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n======= SVM (RBF Kernel) + SMOTE =======\")\n",
    "print(\"Precision:\", precision_score(y_test, svm_preds))\n",
    "print(\"Recall:\", recall_score(y_test, svm_preds))\n",
    "print(\"F1:\", f1_score(y_test, svm_preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, svm_preds))\n",
    "print(\"AUC:\", roc_auc_score(y_test, svm_probs))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, svm_preds))\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, svm_preds)).plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - SVM (RBF) + SMOTE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4499d30-2804-4040-96b4-26504a4a2425",
   "metadata": {},
   "source": [
    "## Comparing Class-Weight Balancing vs SMOTE Oversampling\n",
    "\n",
    "This section compares two imbalance-handling strategies:\n",
    "\n",
    "1. Class-weight balancing (`class_weight='balanced'`)\n",
    "2. SMOTE oversampling (applied only to the training data)\n",
    "\n",
    "Class-weight models act as a baseline.  \n",
    "SMOTE models attempt to create a more balanced representation for the minority class.\n",
    "\n",
    "---\n",
    "\n",
    "## Class-Weight Balanced Models (Baseline)\n",
    "\n",
    "### Logistic Regression (`class_weight='balanced'`, Threshold = 0.29)\n",
    "\n",
    "- **Recall:** 0.907  \n",
    "- **Precision:** 0.517  \n",
    "- **F1-score:** 0.659  \n",
    "- **Accuracy:** 0.672  \n",
    "\n",
    "**Confusion Matrix**\n",
    "```\n",
    "[[273 227]\n",
    " [ 25 243]]\n",
    "```\n",
    "\n",
    "### Random Forest (`class_weight='balanced'`, Threshold = 0.20)\n",
    "\n",
    "- **Recall:** 0.907  \n",
    "- **Precision:** 0.520  \n",
    "- **F1-score:** 0.661  \n",
    "- **Accuracy:** 0.676  \n",
    "\n",
    "**Confusion Matrix**\n",
    "```\n",
    "[[276 224]\n",
    " [ 25 243]]\n",
    "```\n",
    "\n",
    "### Interpretation (Class-Weight Models)\n",
    "\n",
    "- Very high recall indicates the models successfully identify diabetics.\n",
    "- However, both models misclassify a large number of non-diabetics as diabetic (high false positives).\n",
    "- Precision remains low (around 0.52), which is not suitable for clinical usage.\n",
    "- Class-weighting alone leads to over-prediction of the positive class.\n",
    "\n",
    "---\n",
    "\n",
    "## SMOTE-Based Models  \n",
    "*(SMOTE applied safely on the training set only, then scaled, then modeled.)*\n",
    "\n",
    "### Logistic Regression + SMOTE (Threshold = 0.5)\n",
    "\n",
    "- **Precision:** 0.560  \n",
    "- **Recall:** 0.685  \n",
    "- **F1-score:** 0.616  \n",
    "- **Accuracy:** 0.701  \n",
    "- **AUC:** 0.797  \n",
    "\n",
    "**Confusion Matrix**\n",
    "```\n",
    "[[71 29]\n",
    " [17 37]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Random Forest + SMOTE (Threshold = 0.5)\n",
    "\n",
    "- **Precision:** 0.639  \n",
    "- **Recall:** 0.722  \n",
    "- **F1-score:** 0.679  \n",
    "- **Accuracy:** 0.759  \n",
    "- **AUC:** 0.821  \n",
    "\n",
    "**Confusion Matrix**\n",
    "```\n",
    "[[78 22]\n",
    " [15 39]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### XGBoost + SMOTE (Threshold = 0.5)\n",
    "\n",
    "- **Precision:** 0.645  \n",
    "- **Recall:** 0.740  \n",
    "- **F1-score:** 0.689  \n",
    "- **Accuracy:** 0.766  \n",
    "- **AUC:** 0.813  \n",
    "\n",
    "**Confusion Matrix**\n",
    "```\n",
    "[[78 22]\n",
    " [14 40]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### SVM (RBF Kernel) + SMOTE (Threshold = 0.5)\n",
    "\n",
    "- **Precision:** 0.619  \n",
    "- **Recall:** 0.722  \n",
    "- **F1-score:** 0.667  \n",
    "- **Accuracy:** 0.746  \n",
    "- **AUC:** 0.811  \n",
    "\n",
    "**Confusion Matrix**\n",
    "```\n",
    "[[76 24]\n",
    " [15 39]]\n",
    "```\n",
    "\n",
    "### Interpretation (SVM + SMOTE)\n",
    "\n",
    "- SVM performs strongly after SMOTE, achieving recall comparable to Random Forest and strong precision.\n",
    "- Accuracy (0.746) and AUC (0.811) are competitive with RF and XGB.\n",
    "- F1-score is slightly lower than XGBoost but higher than Logistic Regression.\n",
    "- Overall, SVM becomes a viable candidate once SMOTE is used.\n",
    "\n",
    "---\n",
    "\n",
    "## Direct Comparison: Class-Weight vs SMOTE\n",
    "\n",
    "| Model                         | Precision | Recall | F1-score | Accuracy | Notes |\n",
    "|------------------------------|-----------|--------|----------|----------|-------|\n",
    "| Logistic Regression (CW)     | 0.517     | 0.907  | 0.659    | 0.672    | High recall, very low precision |\n",
    "| Random Forest (CW)           | 0.520     | 0.907  | 0.661    | 0.676    | Many false positives |\n",
    "| Logistic Regression + SMOTE  | 0.560     | 0.685  | 0.616    | 0.701    | More balanced performance |\n",
    "| Random Forest + SMOTE        | 0.639     | 0.722  | 0.679    | 0.759    | Strong improvement |\n",
    "| XGBoost + SMOTE              | 0.645     | 0.740  | 0.689    | 0.766    | Best overall metrics |\n",
    "| SVM (RBF) + SMOTE            | 0.619     | 0.722  | 0.667    | 0.746    | Competitive; better than LR+SMOTE |\n",
    "\n",
    "---\n",
    "\n",
    "## Summary and Key Insights\n",
    "\n",
    "- Class-weight balancing gives high recall but fails to control false positives, resulting in low precision and poor real-world usability.\n",
    "- SMOTE improves both recall and precision by creating more realistic synthetic minority samples.\n",
    "- Logistic Regression benefits from SMOTE but still lags behind tree-based models and SVM.\n",
    "- Random Forest and XGBoost show substantial performance improvement after SMOTE.\n",
    "- XGBoost + SMOTE delivers the strongest combination of:\n",
    "  - Precision  \n",
    "  - Recall  \n",
    "  - F1-score  \n",
    "  - Accuracy  \n",
    "  - AUC  \n",
    "- SVM + SMOTE performs surprisingly well and surpasses Logistic Regression + SMOTE, but slightly underperforms XGBoost.\n",
    "\n",
    "**Final takeaway:**  \n",
    "SMOTE is clearly superior to class-weight balancing, and XGBoost + SMOTE is the best-performing model overall. SVM becomes competitive only after SMOTE is applied.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488159e-0853-4fe8-9e3e-5b11377bdd11",
   "metadata": {},
   "source": [
    "# Final Model Comparison Table (All SMOTE Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfcb1af-0a9d-4f9b-9392-3856f4c06317",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create comparison dictionary\n",
    "comparison = {\n",
    "    \"Model\": [\n",
    "        \"Logistic Regression + SMOTE\",\n",
    "        \"Random Forest + SMOTE\",\n",
    "        \"XGBoost + SMOTE\",\n",
    "        \"SVM (RBF) + SMOTE\"\n",
    "    ],\n",
    "    \"Precision\": [\n",
    "        precision_score(y_test, lr_preds),\n",
    "        precision_score(y_test, rf_preds),\n",
    "        precision_score(y_test, xgb_preds),\n",
    "        precision_score(y_test, svm_preds)\n",
    "    ],\n",
    "    \"Recall\": [\n",
    "        recall_score(y_test, lr_preds),\n",
    "        recall_score(y_test, rf_preds),\n",
    "        recall_score(y_test, xgb_preds),\n",
    "        recall_score(y_test, svm_preds)\n",
    "    ],\n",
    "    \"F1-score\": [\n",
    "        f1_score(y_test, lr_preds),\n",
    "        f1_score(y_test, rf_preds),\n",
    "        f1_score(y_test, xgb_preds),\n",
    "        f1_score(y_test, svm_preds)\n",
    "    ],\n",
    "    \"Accuracy\": [\n",
    "        accuracy_score(y_test, lr_preds),\n",
    "        accuracy_score(y_test, rf_preds),\n",
    "        accuracy_score(y_test, xgb_preds),\n",
    "        accuracy_score(y_test, svm_preds)\n",
    "    ],\n",
    "    \"AUC\": [\n",
    "        roc_auc_score(y_test, lr_probs),\n",
    "        roc_auc_score(y_test, rf_probs),\n",
    "        roc_auc_score(y_test, xgb_probs),\n",
    "        roc_auc_score(y_test, svm_probs)\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "display(comparison_df)\n",
    "\n",
    "# ============================================================\n",
    "# Bar Plot for F1-score Comparison\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(\n",
    "    data=comparison_df.sort_values(\"F1-score\", ascending=False),\n",
    "    x=\"Model\", y=\"F1-score\", palette=\"Blues_d\"\n",
    ")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Model Comparison (F1-score) - SMOTE Oversampling\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.xlabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4533f-5a7d-4498-b4ba-979e2e2943e4",
   "metadata": {},
   "source": [
    "### Summary and Discussion\n",
    "\n",
    "Across all models trained using SMOTE, **XGBoost + SMOTE** achieved the strongest overall performance, obtaining the highest F1-score (0.689), accuracy (0.766), and recall (0.740). These results indicate that **XGBoost** provides the best balance between sensitivity and specificity, which is essential for a clinical screening context where both false negatives and false positives must be carefully controlled.  \n",
    "\n",
    "Both **Random Forest + SMOTE** and **SVM (RBF) + SMOTE** also showed substantial gains compared to the class-weight baselines, demonstrating competitive recall and solid overall performance. Their nonlinear decision boundaries benefited significantly from the enriched synthetic samples generated by SMOTE. Meanwhile, **Logistic Regression + SMOTE** produced more balanced performance than its class-weight counterpart, although it still lagged behind the nonlinear models.\n",
    "\n",
    "In comparison, the class-weight-only models (**Logistic Regression (balanced)** and **Random Forest (balanced)**) exhibited very high recall but suffered from excessive false positives, resulting in low precision. This highlights an important limitation of simple class weighting in medical datasets: while it reduces false negatives, it leads to over-diagnosis, making the model less reliable for practical deployment.\n",
    "\n",
    "Overall, these results establish that targeted imbalance handling is crucial for medical prediction tasks. The superior performance of **XGBoost + SMOTE** makes it the most appropriate choice for threshold optimization, interpretability analysis, and eventual deployment in a screening-oriented application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c55de-ec4b-4228-b4b7-b011bd5c9921",
   "metadata": {},
   "source": [
    "## Threshold Tuning for the Final Model (XGBoost + SMOTE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767b619-2157-4389-9237-81deecc68471",
   "metadata": {},
   "source": [
    "### Why Threshold Tuning Is Necessary\n",
    "\n",
    "Most machine learning classifiers produce probability outputs between 0 and 1, but convert these probabilities into class labels using a default threshold of 0.50. While this default value is mathematically convenient, it is rarely optimal—especially in medical diagnostics, where the relative cost of false negatives and false positives is highly unequal.\n",
    "\n",
    "In the context of diabetes prediction, a false negative (a diabetic patient incorrectly classified as healthy) is far more dangerous than a false positive. Therefore, the decision threshold should be carefully optimized to achieve an appropriate balance between sensitivity (recall) and precision. Threshold tuning evaluates model performance across a range of probability cutoffs and identifies the value that maximizes the clinical objective—commonly high recall with an acceptable level of precision, or the highest overall F1-score.\n",
    "\n",
    "### Why Threshold Tuning Is Performed Only on the Best Model\n",
    "\n",
    "Threshold tuning is computationally intensive and is meaningful only for the final selected model. Since earlier experiments showed that **XGBoost + SMOTE** consistently achieved the strongest performance across accuracy, recall, precision, F1-score, and AUC, it is chosen as the final candidate for deployment. Running threshold optimization on multiple models would add unnecessary complexity without offering additional practical value, because only the best-performing model will be used in the final application.\n",
    "\n",
    "Focusing threshold tuning exclusively on **XGBoost + SMOTE** ensures that the optimization effort is directed where it matters most: refining the performance of the clinically relevant final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be76ec-09f0-443f-a396-ea7d942fb788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Threshold Tuning for XGBoost + SMOTE\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# 1. Get predicted probabilities\n",
    "xgb_probs = xgb.predict_proba(X_test_s)[:, 1]\n",
    "\n",
    "# 2. Generate Precision-Recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, xgb_probs)\n",
    "\n",
    "# 3. Compute F1-score for each threshold (avoid division by zero)\n",
    "f1_scores = (2 * precision * recall) / (precision + recall + 1e-9)\n",
    "\n",
    "# 4. Identify the threshold that maximizes F1-score\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "print(\"Best Threshold (max F1):\", best_threshold)\n",
    "print(\"Best F1-score:\", best_f1)\n",
    "\n",
    "# ============================================================\n",
    "# Plot Precision-Recall Curve\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(thresholds, precision[:-1], label=\"Precision\")\n",
    "plt.plot(thresholds, recall[:-1], label=\"Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision and Recall vs Threshold (XGBoost)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# Plot F1 vs Threshold Curve\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(thresholds, f1_scores[:-1], label=\"F1-score\", color='purple')\n",
    "plt.axvline(best_threshold, color='red', linestyle='--', label=f\"Best Threshold = {best_threshold:.2f}\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.title(\"F1-score vs Threshold (XGBoost)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# Evaluate Performance at Best Threshold\n",
    "# ============================================================\n",
    "\n",
    "xgb_preds_tuned = (xgb_probs >= best_threshold).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, xgb_preds_tuned)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"\\nConfusion Matrix at Tuned Threshold:\\n\", cm)\n",
    "print(\"True Negative:\", tn)\n",
    "print(\"False Positive:\", fp)\n",
    "print(\"False Negative:\", fn)\n",
    "print(\"True Positive:\", tp)\n",
    "\n",
    "print(\"\\nMetrics at Tuned Threshold:\")\n",
    "print(\"Precision:\", precision_score(y_test, xgb_preds_tuned))\n",
    "print(\"Recall:\", recall_score(y_test, xgb_preds_tuned))\n",
    "print(\"F1-score:\", f1_score(y_test, xgb_preds_tuned))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, xgb_preds_tuned))\n",
    "print(\"AUC:\", roc_auc_score(y_test, xgb_probs))\n",
    "\n",
    "# ============================================================\n",
    "# Plot Confusion Matrix (Tuned Threshold)\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(f\"Confusion Matrix - XGBoost (Threshold = {best_threshold:.2f})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a9c1c8-6bc0-4334-9bce-6a30455d924e",
   "metadata": {},
   "source": [
    "### Interpretation of Threshold Tuning Results\n",
    "\n",
    "Threshold tuning was performed to identify the probability cutoff that maximizes the clinical balance between false positives and false negatives. Using the Precision–Recall and F1-score curves, the optimal threshold for **XGBoost + SMOTE** was automatically identified as **0.51**, which achieved the highest F1-score (0.690). This value represents the point where the model most effectively balances precision and recall.\n",
    "\n",
    "At the tuned threshold of **0.51**, the model produced the following confusion matrix:\n",
    "\n",
    "```\n",
    "[[78 22]\n",
    " [14 40]]\n",
    "```\n",
    "\n",
    "- **True Negatives:** 78  \n",
    "- **False Positives:** 22  \n",
    "- **False Negatives:** 14  \n",
    "- **True Positives:** 40  \n",
    "\n",
    "### Performance at Tuned Threshold\n",
    "\n",
    "- **Precision:** 0.645  \n",
    "- **Recall:** 0.740  \n",
    "- **F1-score:** 0.690  \n",
    "- **Accuracy:** 0.766  \n",
    "- **AUC:** 0.813  \n",
    "\n",
    "### Clinical Interpretation\n",
    "\n",
    "Increasing the threshold above 0.50 typically increases precision at the cost of recall; however, in this case, the tuned threshold of 0.51 keeps recall high (0.74) while simultaneously improving precision compared to the class-weighted and defaultthreshold models. This indicates that the model becomes more selective in predicting diabetes while still capturing the majority of true diabetic cases.\n",
    "\n",
    "A **recall of 0.74** means the model correctly identifies 74% of diabetic individuals, reducing the number of potentially dangerous false negatives. Meanwhile, a **precision of 0.645** ensures that most individuals predicted as diabetic truly belong to the positive class, minimizing unnecessary anxiety and follow-up tests.\n",
    "\n",
    "The tuned threshold therefore provides a clinically meaningful balance:  \n",
    "- It **reduces false negatives**, which is crucial in diabetes screening.  \n",
    "- It **controls false positives**, improving the model's usefulness in real clinical workflows.  \n",
    "- It maintains strong overall discrimination (**AUC = 0.813**).  \n",
    "\n",
    "Overall, threshold tuning significantly enhances the model's practical reliability. The final performance metrics support the selection of **XGBoost + SMOTE (Threshold = 0.51)** as the model to be carried forward for interpretability analysis and deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcfdf36-3365-42ed-b3ae-474988528ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Dual-Threshold System: Balanced Mode and High-Sensitivity Mode\n",
    "# ============================================================\n",
    "\n",
    "# --- Balanced Threshold (already found from F1) ---\n",
    "balanced_threshold = best_threshold   # ~0.51\n",
    "print(\"Balanced Threshold (F1-optimal):\", balanced_threshold)\n",
    "\n",
    "# ============================================================\n",
    "# Compute HIGH-SENSITIVITY Threshold (Recall ≥ 0.85)\n",
    "# ============================================================\n",
    "\n",
    "desired_recall = 0.85\n",
    "high_recall_threshold = None\n",
    "\n",
    "# thresholds has length = len(precision)-1, align recall & thresholds\n",
    "full_thresholds = np.append(thresholds, 1)\n",
    "\n",
    "# Recall increases as threshold decreases, so reverse-scan:\n",
    "for r, t in zip(recall[::-1], full_thresholds[::-1]):\n",
    "    if r >= desired_recall:\n",
    "        high_recall_threshold = t\n",
    "        break\n",
    "\n",
    "# If no threshold reaches recall ≥ 0.85\n",
    "if high_recall_threshold is None:\n",
    "    print(\"\\n⚠ Model cannot reach Recall ≥ 0.85 at any reasonable threshold.\")\n",
    "    print(\"Selecting highest-recall threshold with precision > 0.\")\n",
    "\n",
    "    # Choose best fallback threshold\n",
    "    best_r = 0\n",
    "    best_t = None\n",
    "\n",
    "    for r, p, t in zip(recall, precision, full_thresholds):\n",
    "        if p > 0 and r > best_r:\n",
    "            best_r = r\n",
    "            best_t = t\n",
    "\n",
    "    high_recall_threshold = best_t\n",
    "    print(\"Fallback High-Sensitivity Threshold:\", high_recall_threshold)\n",
    "    print(\"Recall at fallback threshold:\", best_r)\n",
    "else:\n",
    "    print(\"\\nHigh-Sensitivity Threshold (Recall ≥ 0.85):\", high_recall_threshold)\n",
    "\n",
    "# ============================================================\n",
    "# BALANCED MODE EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "xgb_preds_balanced = (xgb_probs >= balanced_threshold).astype(int)\n",
    "cm_balanced = confusion_matrix(y_test, xgb_preds_balanced)\n",
    "\n",
    "print(\"\\n=== BALANCED MODE (Threshold =\", round(balanced_threshold, 2), \") ===\")\n",
    "print(\"Confusion Matrix:\\n\", cm_balanced)\n",
    "print(\"Precision:\", precision_score(y_test, xgb_preds_balanced))\n",
    "print(\"Recall:\", recall_score(y_test, xgb_preds_balanced))\n",
    "print(\"F1-score:\", f1_score(y_test, xgb_preds_balanced))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, xgb_preds_balanced))\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_balanced).plot(cmap=\"Blues\")\n",
    "plt.title(f\"Balanced Mode - XGBoost (Threshold = {balanced_threshold:.2f})\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# HIGH-SENSITIVITY MODE EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "xgb_preds_highrecall = (xgb_probs >= high_recall_threshold).astype(int)\n",
    "cm_highrecall = confusion_matrix(y_test, xgb_preds_highrecall)\n",
    "\n",
    "print(\"\\n=== HIGH-SENSITIVITY MODE (Threshold =\", round(high_recall_threshold, 2), \") ===\")\n",
    "print(\"Confusion Matrix:\\n\", cm_highrecall)\n",
    "print(\"Precision:\", precision_score(y_test, xgb_preds_highrecall))\n",
    "print(\"Recall:\", recall_score(y_test, xgb_preds_highrecall))\n",
    "print(\"F1-score:\", f1_score(y_test, xgb_preds_highrecall))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, xgb_preds_highrecall))\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_highrecall).plot(cmap=\"Blues\")\n",
    "plt.title(f\"High-Sensitivity Mode - XGBoost (Threshold = {high_recall_threshold:.2f})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997fe00c-cb98-4e98-937c-a9616c79b570",
   "metadata": {},
   "source": [
    "### Dual-Threshold Decision System for Clinical Deployment\n",
    "\n",
    "Medical models often require different operating points depending on the clinical setting. A single fixed decision threshold cannot satisfy every scenario because the cost of false negatives and false positives varies by patient population. To address this, we implement a dual-threshold design that supports two clinically meaningful modes:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Balanced Mode (Threshold = 0.51)**  \n",
    "This threshold was selected using the F1-maximizing criterion, providing the strongest balance between precision and recall.\n",
    "\n",
    "**Confusion Matrix**\n",
    "```\n",
    "[[78 22]\n",
    " [14 40]]\n",
    "```\n",
    "\n",
    "- **Precision:** 0.645  \n",
    "- **Recall:** 0.740  \n",
    "- **F1-score:** 0.689  \n",
    "- **Accuracy:** 0.766  \n",
    "\n",
    "Balanced mode is suitable for:\n",
    "\n",
    "- General population screening  \n",
    "- Primary care clinics  \n",
    "- Situations where both false positives and false negatives carry moderate clinical cost  \n",
    "\n",
    "This mode reduces unnecessary follow-up tests while still identifying a large proportion of true diabetic patients.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. High-Sensitivity Mode (Threshold = 0.19)**  \n",
    "To support high-risk groups, we identify a threshold that achieves **high recall** while preserving meaningful precision. The threshold of **0.19** achieves:\n",
    "\n",
    "**Confusion Matrix**\n",
    "```\n",
    "[[63 37]\n",
    " [ 8 46]]\n",
    "```\n",
    "\n",
    "- **Precision:** 0.554  \n",
    "- **Recall:** 0.852  \n",
    "- **F1-score:** 0.672  \n",
    "- **Accuracy:** 0.708  \n",
    "\n",
    "This mode drastically reduces false negatives—from 14 in Balanced Mode down to just 8—meaning fewer diabetic patients are missed. This is critical because false negatives carry the highest clinical risk.\n",
    "\n",
    "High-Sensitivity mode is appropriate for:\n",
    "\n",
    "- High-risk patients (family history, obesity, hypertension)  \n",
    "- Hospital triage  \n",
    "- Early detection programs  \n",
    "- Public health outreach deployments  \n",
    "- Screening in rural or resource-limited regions  \n",
    "\n",
    "The trade-off is an increase in false positives (22 → 37), but this is acceptable in clinical screening where early detection outweighs the cost of additional confirmatory tests.\n",
    "\n",
    "---\n",
    "\n",
    "## **Clinical Importance of the Dual-Threshold System**\n",
    "\n",
    "This two-threshold framework reflects real clinical practice:\n",
    "\n",
    "- **Balanced Mode** is used for typical diagnosis where efficiency and accuracy are both required.  \n",
    "- **High-Sensitivity Mode** prioritizes patient safety by minimizing false negatives, even if precision decreases.\n",
    "\n",
    "Implementing dual operating points demonstrates:\n",
    "\n",
    "- Awareness of real-world clinical risk trade-offs  \n",
    "- Strong deployment-oriented thinking  \n",
    "- Adaptability of the model to different healthcare environments  \n",
    "- Understanding that medical ML is not “one-threshold-fits-all”  \n",
    "\n",
    "This design significantly strengthens the model’s usability and shows maturity expected in advanced research internships like the Max Planck Institute.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
